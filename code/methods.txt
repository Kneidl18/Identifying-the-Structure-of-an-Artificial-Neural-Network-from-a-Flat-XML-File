Idea:
  - A neuron's layer can be determined by following links backwards
    (the last neuron as the sink) until no more link is found. The number
    of steps equals the layer.
Result:
  - False, because there is a neuron (star4_24_12, id=2230062), that despite
    not being part of the input layer does not have a corresponding link where
    it acts as the sink.

Idea:
  - We can group neurons into layers by looking at the difference between two neighbouring neurons.
Result:
  - False, because there are neighbouring neurons of the same layer with drastically different ids,
    making this kind of grouping incorrect.

Idea:
  - We can determine all layers that are convolutional layers by checking the sinks' layer
Assumption/Requirement:
  - We know which layer a neuron belongs to
  - Only convolutional layers use MultiLinks
Result:
  - Works for all samples

Idea:
  - We can accurately assign all neurons to their corresponding layer,
    i.e. we can determine each neuron's "ly" attribute.
Assumption/Requirement:
  - We know the layers (level & size)
Result:
  - works for all samples

Idea:
  - We can identify all maps of a given convolutional layer, by doing the following: A map ends (and a new map starts)
    when the intersection of the sets of the two MultiLinkBlocks is empty.
Assumption/Requirement:
  - None
Result
  - 100% correct results for star3*, the other two are nearly fully correct. I am suspecting that the incorrect results come from negative padding
    (how would you even know that it was used)

Idea:
  - By creating a set of all multilinks used in a single map, we can derive the size of the kernel by taking
    the square root of the set's cardinality.
Assumption/Requirement:
  - None
Result:
  - Works always with one exception: in star7_109_15 layer 2 it states that the kernel is 8x8, but with this
    method it claims that it's actually 16x16. Perhaps we need to analyse the file directly to be able to
    tell why exactly this is the case.



